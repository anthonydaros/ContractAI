# ==============================================
# CENTRALIZED CONFIGURATION
# Copy this file to .env
# ==============================================

# --- Backend Settings ---
ENVIRONMENT=production
API_HOST=0.0.0.0
API_PORT=8000
API_DEBUG=false
ALLOWED_ORIGINS=http://localhost:3000

# --- Ollama / AI Settings ---
# Use host.docker.internal to access Ollama on the host machine from Docker
OLLAMA_BASE_URL=http://host.docker.internal:11434
# If using Linux, use: http://172.17.0.1:11434

OLLAMA_API_KEY=ollama
OLLAMA_MODEL=llama3
OLLAMA_EMBEDDING_MODEL=nomic-embed-text
OLLAMA_TEMPERATURE=0.3

# --- Frontend Settings ---
NODE_ENV=production
# This URL is used by the Next.js client (browser)
NEXT_PUBLIC_API_URL=http://localhost:8000
